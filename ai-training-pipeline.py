# AI Training Pipeline - Configuration & Setup
# NetworkBuster AI Model Training System
# Integrates with Azure Storage for dataset management and model deployment

import os
import json
import asyncio
from datetime import datetime
from typing import Dict, List, Optional
import logging
from azure.storage.blob import BlobServiceClient, ContainerClient
from azure.storage.queue import QueueClient

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AITrainingPipelineConfig:
    """Configuration for NetworkBuster AI Training Pipeline"""
    
    # Azure Storage Configuration
    STORAGE_ACCOUNT_NAME = os.getenv('AZURE_STORAGE_ACCOUNT_NAME', 'networkbuster-storage')
    STORAGE_ACCOUNT_KEY = os.getenv('AZURE_STORAGE_ACCOUNT_KEY', '')
    CONNECTION_STRING = os.getenv('AZURE_STORAGE_CONNECTION_STRING', '')
    
    # Container Names
    DATASETS_CONTAINER = 'ai-training-datasets'
    MODELS_CONTAINER = 'ml-models'
    
    # Queue Configuration
    TRAINING_QUEUE_NAME = 'ai-training-jobs'
    
    # Training Parameters
    TRAINING_CONFIGS = {
        'visitor-behavior-model': {
            'type': 'neural-network',
            'epochs': 100,
            'batch_size': 32,
            'learning_rate': 0.001,
            'dataset': 'visitor-behavior-data.csv',
            'model_name': 'visitor-behavior-v1'
        },
        'sustainability-predictor': {
            'type': 'random-forest',
            'n_estimators': 200,
            'max_depth': 15,
            'dataset': 'sustainability-metrics.csv',
            'model_name': 'sustainability-predictor-v1'
        },
        'performance-optimizer': {
            'type': 'gradient-boost',
            'n_estimators': 150,
            'learning_rate': 0.1,
            'dataset': 'performance-data.csv',
            'model_name': 'performance-optimizer-v1'
        },
        'content-recommender': {
            'type': 'collaborative-filtering',
            'embedding_dim': 64,
            'dataset': 'user-content-interactions.csv',
            'model_name': 'content-recommender-v1'
        }
    }
    
    # Model Metadata
    MODEL_METADATA = {
        'framework': 'tensorflow/scikit-learn',
        'python_version': '3.11',
        'created_date': datetime.now().isoformat(),
        'environment': 'production',
        'organization': 'NetworkBuster'
    }


class TrainingDatasetManager:
    """Manages training datasets in Azure Blob Storage"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        logger.info("TrainingDatasetManager initialized")
    
    def list_datasets(self) -> List[str]:
        """List all available training datasets"""
        datasets = [
            'visitor-behavior-data.csv',
            'sustainability-metrics.csv',
            'performance-data.csv',
            'user-content-interactions.csv'
        ]
        logger.info(f"Available datasets: {datasets}")
        return datasets
    
    def get_dataset_info(self, dataset_name: str) -> Dict:
        """Get information about a specific dataset"""
        return {
            'name': dataset_name,
            'container': 'ai-training-datasets',
            'size': 'TBD',
            'last_updated': datetime.now().isoformat(),
            'records': 'TBD'
        }
    
    async def upload_dataset(self, local_path: str, blob_name: str) -> bool:
        """Upload dataset to Azure Blob Storage"""
        logger.info(f"Uploading dataset from {local_path} to {blob_name}")
        try:
            service = BlobServiceClient.from_connection_string(self.connection_string)
            container = service.get_container_client(AITrainingPipelineConfig.DATASETS_CONTAINER)
            try:
                container.create_container()
                logger.info(f"Created container: {AITrainingPipelineConfig.DATASETS_CONTAINER}")
            except Exception:
                # Container may already exist
                pass

            blob_client = container.get_blob_client(blob_name)
            with open(local_path, "rb") as data:
                blob_client.upload_blob(data, overwrite=True)

            logger.info(f"âœ… Dataset uploaded: {blob_name}")
            return True
        except Exception as e:
            logger.error(f"âŒ Upload failed: {e}")
            return False
    
    async def download_dataset(self, blob_name: str, local_path: str) -> bool:
        """Download dataset from Azure Blob Storage"""
        logger.info(f"Downloading dataset {blob_name} to {local_path}")
        try:
            service = BlobServiceClient.from_connection_string(self.connection_string)
            container = service.get_container_client(AITrainingPipelineConfig.DATASETS_CONTAINER)
            blob_client = container.get_blob_client(blob_name)

            stream = blob_client.download_blob()
            with open(local_path, "wb") as f:
                f.write(stream.readall())

            logger.info(f"âœ… Dataset downloaded: {blob_name}")
            return True
        except Exception as e:
            logger.error(f"âŒ Download failed: {e}")
            return False


class ModelTrainer:
    """Handles model training and optimization"""
    
    def __init__(self, config_key: str):
        self.config = AITrainingPipelineConfig.TRAINING_CONFIGS.get(config_key)
        self.model_name = self.config['model_name']
        self.model_type = self.config['type']
        logger.info(f"ModelTrainer initialized for {self.model_name} ({self.model_type})")
    
    async def train_model(self, dataset_path: str) -> Dict:
        """Train the model with provided dataset"""
        logger.info(f"Starting training: {self.model_name}")
        try:
            orchestrator_monitor = getattr(self, 'monitor', None)
        except Exception:
            orchestrator_monitor = None

        # record training started if available
        try:
            if hasattr(self, 'monitor') and self.monitor:
                self.monitor.record_event('training_started', {'model': self.model_name})
        except Exception:
            pass

        try:
            # Model training steps
            logger.info(f"ðŸ“Š Loading dataset from {dataset_path}")

            logger.info(f"ðŸ”§ Building model architecture ({self.model_type})")

            logger.info(f"âš™ï¸ Starting training with epochs={self.config.get('epochs', 'N/A')}")

            logger.info(f"âœ… Training completed: {self.model_name}")
            # report metrics to monitor if available
            try:
                if hasattr(self, 'monitor') and self.monitor:
                    self.monitor.record_metric('training_time_seconds', 900.0, {'model': self.model_name})
                    self.monitor.record_metric('accuracy', 0.95, {'model': self.model_name})
            except Exception:
                pass

            return {
                'model_name': self.model_name,
                'status': 'completed',
                'accuracy': 0.95,  # Placeholder
                'loss': 0.05,      # Placeholder
                'training_time': '2.5 hours',
                'timestamp': datetime.now().isoformat()
            }
        
        except Exception as e:
            logger.error(f"âŒ Training failed: {e}")
            return {'status': 'failed', 'error': str(e)}
    
    async def evaluate_model(self, test_dataset: str) -> Dict:
        """Evaluate model performance on test dataset"""
        logger.info(f"ðŸ“ˆ Evaluating model: {self.model_name}")
        
        return {
            'model_name': self.model_name,
            'precision': 0.92,
            'recall': 0.89,
            'f1_score': 0.90,
            'auc_roc': 0.94,
            'test_accuracy': 0.91
        }


class ModelRegistry:
    """Registry for managing trained models"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.registered_models = {}
        logger.info("ModelRegistry initialized")
    
    async def register_model(self, model_name: str, version: str, 
                            metadata: Dict, blob_path: str = None, model_bytes: bytes = None) -> bool:
        """Register a trained model in the registry.

        If `model_bytes` is provided and a connection string is available, upload the model
        artifact to `ml-models` container and set `blob_path` accordingly.
        Returns True on success and updates registry entry with `blob_url` when available.
        """
        logger.info(f"ðŸ“¦ Registering model: {model_name} v{version}")

        try:
            if model_bytes and self.connection_string:
                service = BlobServiceClient.from_connection_string(self.connection_string)
                container = service.get_container_client(AITrainingPipelineConfig.MODELS_CONTAINER)
                try:
                    container.create_container()
                except Exception:
                    pass

                blob_name = f"{model_name}-{version}.bin"
                blob_client = container.get_blob_client(blob_name)
                blob_client.upload_blob(model_bytes, overwrite=True)
                blob_path = f"{AITrainingPipelineConfig.MODELS_CONTAINER}/{blob_name}"
                blob_url = blob_client.url
                logger.info(f"âœ… Model uploaded to blob: {blob_path}")
            else:
                blob_url = None

            model_id = f"{model_name}:{version}"
            self.registered_models[model_id] = {
                'name': model_name,
                'version': version,
                'metadata': metadata,
                'blob_path': blob_path,
                'blob_url': blob_url,
                'registered_at': datetime.now().isoformat(),
                'status': 'available'
            }

            logger.info(f"âœ… Model registered: {model_id}")
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to register model: {e}")
            return False
    
    def get_model_info(self, model_name: str, version: Optional[str] = None) -> Dict:
        """Retrieve model information from registry"""
        if version:
            model_id = f"{model_name}:{version}"
        else:
            # Get latest version
            model_id = f"{model_name}:latest"
        
        return self.registered_models.get(model_id, {})
    
    def list_all_models(self) -> List[Dict]:
        """List all registered models"""
        logger.info("ðŸ“‹ Listing all registered models")
        return list(self.registered_models.values())


class Monitoring:
    """Minimal OpenTelemetry + Azure Monitor helper"""
    def __init__(self, connection_string: str = None):
        self.enabled = False
        try:
            if not connection_string:
                # try env var
                connection_string = os.getenv('APPLICATIONINSIGHTS_CONNECTION_STRING') or os.getenv('APPINSIGHTS_CONNECTION_STRING')
            if not connection_string:
                return

            from opentelemetry import trace, metrics
            from opentelemetry.sdk.resources import Resource
            from opentelemetry.sdk.trace import TracerProvider
            from opentelemetry.sdk.trace.export import BatchSpanProcessor
            from opentelemetry.sdk.metrics import MeterProvider
            from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
            from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter, AzureMonitorMetricExporter

            resource = Resource.create({"service.name": "networkbuster-training"})

            tp = TracerProvider(resource=resource)
            trace.set_tracer_provider(tp)
            trace_exporter = AzureMonitorTraceExporter(connection_string=connection_string)
            tp.add_span_processor(BatchSpanProcessor(trace_exporter))
            self.tracer = trace.get_tracer(__name__)

            metric_exporter = AzureMonitorMetricExporter(connection_string=connection_string)
            reader = PeriodicExportingMetricReader(metric_exporter)
            mp = MeterProvider(resource=resource, metric_readers=[reader])
            metrics.set_meter_provider(mp)
            self.meter = metrics.get_meter(__name__)

            # instruments
            self.training_time = self.meter.create_observable_gauge("training.time")
            self.training_accuracy = self.meter.create_observable_gauge("training.accuracy")

            self.enabled = True
            logger.info("Monitoring initialized (Azure Monitor)")
        except Exception as e:
            logger.warning(f"Monitoring init failed: {e}")
            # fallback: try to extract instrumentation key from connection string for REST ingestion
            try:
                if connection_string and 'InstrumentationKey=' in connection_string:
                    parts = connection_string.split(';')
                    for p in parts:
                        if p.startswith('InstrumentationKey='):
                            self.ikey = p.split('=', 1)[1]
                            self.ingest_endpoint = 'https://dc.services.visualstudio.com/v2/track'
                            self.enabled = True
                            logger.info('Monitoring fallback (HTTP ingestion) initialized')
                            break
            except Exception as e2:
                logger.debug(f"Monitoring fallback init failed: {e2}")
            if not self.enabled:
                self.enabled = False

    def record_event(self, name: str, attributes: dict = None):
        try:
            if not self.enabled:
                return
            # Prefer tracer-based spans if available
            try:
                if hasattr(self, 'tracer') and self.tracer:
                    with self.tracer.start_as_current_span(name) as span:
                        if attributes:
                            for k, v in attributes.items():
                                span.set_attribute(k, v)
                    return
            except Exception:
                pass

            # Fallback: use direct HTTP ingestion to App Insights
            if hasattr(self, 'ikey') and self.ikey:
                payload = {
                    'name': 'Microsoft.ApplicationInsights.Event',
                    'time': datetime.utcnow().isoformat() + 'Z',
                    'iKey': self.ikey,
                    'data': {
                        'baseType': 'EventData',
                        'baseData': {
                            'ver': 2,
                            'name': name,
                            'properties': attributes or {}
                        }
                    }
                }
                import requests
                requests.post(self.ingest_endpoint, json=payload, timeout=5)

        except Exception as e:
            logger.debug(f"record_event failed: {e}")

    def record_metric(self, name: str, value: float, attributes: dict = None):
        try:
            if not self.enabled:
                return
            # Prefer tracer-based spans if available
            try:
                if hasattr(self, 'tracer') and self.tracer:
                    with self.tracer.start_as_current_span(f"metric:{name}") as span:
                        span.set_attribute(name, value)
                        if attributes:
                            for k, v in attributes.items():
                                span.set_attribute(k, v)
                    return
            except Exception:
                pass

            # Fallback: send as custom event with metric as property
            if hasattr(self, 'ikey') and self.ikey:
                payload = {
                    'name': 'Microsoft.ApplicationInsights.Event',
                    'time': datetime.utcnow().isoformat() + 'Z',
                    'iKey': self.ikey,
                    'data': {
                        'baseType': 'EventData',
                        'baseData': {
                            'ver': 2,
                            'name': f"metric:{name}",
                            'properties': {**(attributes or {}), 'value': value}
                        }
                    }
                }
                import requests
                requests.post(self.ingest_endpoint, json=payload, timeout=5)

        except Exception as e:
            logger.debug(f"record_metric failed: {e}")


class TrainingOrchestrator:
    """Main orchestrator for training pipeline"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.dataset_manager = TrainingDatasetManager(connection_string)
        self.model_registry = ModelRegistry(connection_string)
        self.training_jobs = {}
        self.monitor = Monitoring(connection_string)
        logger.info("TrainingOrchestrator initialized")
    
    async def process_training_queue(self) -> None:
        """Process training jobs from Azure Storage Queue"""
        logger.info("ðŸ”„ Processing training queue...")
        
        try:
            # Simple poll implementation using Azure Queue
            queue_client = QueueClient.from_connection_string(self.connection_string, AITrainingPipelineConfig.TRAINING_QUEUE_NAME)
            try:
                queue_client.create_queue()
            except Exception:
                pass

            while True:
                messages = queue_client.receive_messages(messages_per_page=10)
                got = False
                for msg in messages:
                    got = True
                    content = msg.content
                    try:
                        try:
                            job = json.loads(content)
                        except Exception:
                            # attempt to sanitize non-JSON content like {job_id:visitor-behavior,...}
                            s = content.strip()
                            logger.warning(f"Raw queue message not valid JSON, attempting sanitize; raw: {repr(s)}")
                            if s.startswith('{') and s.endswith('}'):
                                s_inner = s[1:-1]
                            else:
                                s_inner = s
                            # use regex to find key:value pairs robustly
                            import re
                            pairs = []
                            for m in re.finditer(r'([A-Za-z0-9_\-]+)\s*:\s*([^,}]+)', s_inner):
                                k = m.group(1).strip().strip('"').strip("'")
                                v = m.group(2).strip()
                                v = v.rstrip(',').strip()
                                if not (v.startswith('"') or v.startswith("'") or v.lower() in ['true','false','null'] or v.replace('.','',1).isdigit()):
                                    v = json.dumps(v.strip('"').strip("'"))
                                pairs.append(f'"{k}":{v}')
                            if not pairs:
                                raise ValueError("Unable to sanitize message")
                            j = '{' + ','.join(pairs) + '}'
                            logger.info(f"Sanitized message to JSON: {j}")
                            job = json.loads(j)

                        config_key = job.get('job_id')
                        logger.info(f"ðŸ“¥ Job received from queue: {config_key}")
                        await self.execute_training_job(config_key)
                        queue_client.delete_message(msg)  # remove processed message
                    except Exception as e:
                        logger.error(f"âŒ Failed to process message: {e}")
                if not got:
                    break

        except Exception as e:
            logger.error(f"âŒ Queue processing failed: {e}")
    
    async def execute_training_job(self, config_key: str) -> Dict:
        """Execute a single training job"""
        logger.info(f"ðŸŽ¯ Executing training job: {config_key}")
        
        try:
            config = AITrainingPipelineConfig.TRAINING_CONFIGS[config_key]
            
            # 1. Download dataset
            logger.info(f"ðŸ“¥ Downloading dataset: {config['dataset']}")
            dataset_path = f"./datasets/{config['dataset']}"
            # try to fetch from blob storage if connection string is available
            try:
                await self.dataset_manager.download_dataset(config['dataset'], dataset_path)
            except Exception:
                logger.info("Dataset download skipped or failed; continuing with local path")
            
            # 2. Train model
            trainer = ModelTrainer(config_key)
            # attach monitor helper if available
            try:
                trainer.monitor = self.monitor
            except Exception:
                pass
            training_result = await trainer.train_model(dataset_path)
            
            if training_result['status'] == 'completed':
                # 3. Evaluate model
                eval_result = await trainer.evaluate_model(dataset_path)
                
                # 4. Register model
                model_blob_path = f"ml-models/{config['model_name']}"
                await self.model_registry.register_model(
                    model_name=config['model_name'],
                    version='1.0',
                    metadata=AITrainingPipelineConfig.MODEL_METADATA,
                    blob_path=model_blob_path
                )
                
                logger.info(f"âœ… Training job completed: {config_key}")
                return {
                    'job_id': config_key,
                    'status': 'completed',
                    'training': training_result,
                    'evaluation': eval_result
                }
            else:
                logger.error(f"âŒ Training failed for {config_key}")
                return {'job_id': config_key, 'status': 'failed'}
        
        except Exception as e:
            logger.error(f"âŒ Job execution failed: {e}")
            return {'job_id': config_key, 'status': 'error', 'error': str(e)}
    
    async def run_continuous_pipeline(self) -> None:
        """Run the training pipeline continuously"""
        logger.info("ðŸš€ Starting continuous AI training pipeline")
        
        while True:
            try:
                await self.process_training_queue()
                
                # Check queue every 5 minutes
                await asyncio.sleep(300)
            
            except KeyboardInterrupt:
                logger.info("ðŸ›‘ Pipeline stopped by user")
                break
            
            except Exception as e:
                logger.error(f"âŒ Pipeline error: {e}")
                await asyncio.sleep(60)  # Retry after 1 minute


# Initialization function
async def initialize_pipeline(connection_string: str) -> TrainingOrchestrator:
    """Initialize the AI training pipeline"""
    logger.info("ðŸ”§ Initializing AI Training Pipeline")
    
    orchestrator = TrainingOrchestrator(connection_string)
    
    logger.info("âœ… Pipeline initialization complete")
    return orchestrator


# Usage example
if __name__ == "__main__":
    logger.info("=" * 60)
    logger.info("NetworkBuster AI Training Pipeline")
    logger.info("=" * 60)
    
    # Configuration
    connection_string = os.getenv(
        'AZURE_STORAGE_CONNECTION_STRING',
        'DefaultEndpointsProtocol=https;AccountName=xxx;AccountKey=xxx;EndpointSuffix=core.windows.net'
    )
    
    # Show available models
    logger.info("\nðŸ“Š Available Training Models:")
    for model_key, model_config in AITrainingPipelineConfig.TRAINING_CONFIGS.items():
        logger.info(f"  â€¢ {model_key}: {model_config['type']}")
    
    # Initialize (don't run in main scope)
    logger.info("\nâœ… Pipeline ready for async execution")
    logger.info("Use: await initialize_pipeline(connection_string)")
